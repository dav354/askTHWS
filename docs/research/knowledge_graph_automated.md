## 1. Einleitung

Ein **Knowledge Graph** (Wissensgraph) wandelt die unstrukturierten Inhalte einer Website in ein strukturiertes Netzwerk aus Wissen um. Dadurch können Informationen über Zusammenhänge explizit gemacht und leichter abgefragt werden. Insbesondere für Websites wie **fiw.thws.de** (Fakultät Informatik und Wirtschaftsinformatik der THWS) bietet ein Wissensgraph Mehrwert: Zum Beispiel ermöglicht er **semantische Suche** und **intelligente Navigation**, da Nutzer Anfragen auf konzeptueller Ebene stellen können (z. B. *„Zeige alle Module im Studiengang Informatik“*). Ebenso kann ein Wissensgraph die Grundlage für **QA-Chatbots** bilden, die Nutzern präzise Antworten liefern, indem sie strukturierte Fakten anstelle von Stichwortsuche verwenden. Durch die maschinenlesbare Darstellung der Inhalte lassen sich außerdem **Empfehlungssysteme** (z. B. verwandte Studiengänge oder Module) realisieren. Kurz gesagt: Ein Wissensgraph erhöht die **Auffindbarkeit und Wiederverwendbarkeit** von Web-Inhalten und schafft die Basis für fortgeschrittene Anwendungen der Datenanalyse.

## 2. Grundlagen / Was ist ein Website-Knowledge-Graph?

Ein Website-Knowledge-Graph ist eine **strukturierte Darstellung** der Entitäten (Objekte) und Beziehungen, die in den Inhalten einer Website vorkommen. Anstatt Informationen nur in Seiten und Absätzen vorzuhalten, werden sie als Knoten (Entitäten) und Kanten (Relationen) in einem Graphenmodell organisiert. Das Ziel ist, Wissen explizit und vernetzt abzubilden, sodass Computer es **verstehen und verarbeiten** können.

**Begriffsklärung:** Ein *Wissensgraph* wurde erstmals 2012 von Google populär gemacht und beschreibt ein Netz aus Knoten (z. B. Personen, Orte, Objekte) und Kanten (semantische Beziehungen zwischen diesen Knoten). Im Kontext einer Hochschul-Website wie fiw.thws.de könnten **typische Knoten-Typen** folgende sein:

- **Studiengang:** z. B. *Informatik (B.Eng.)*, *E-Commerce (B.Sc.)*, *Wirtschaftsinformatik (B.Sc.)*. Jeder Studiengang ist ein Knoten.
- **Modul:** einzelne Lehrveranstaltungen oder Module, die in Studiengängen vorkommen (ggf. aus Modulhandbüchern extrahiert).
- **Ansprechpartner:** Personen wie Professoren, Mitarbeitende oder Studiengangsleiter. Diese sind als Knoten *Person* modellierbar, mit Attributen (E-Mail, Telefon etc.).
- **Frist/Termin:** Wichtige Termine wie Bewerbungsfristen, Prüfungszeiträume oder Anmeldefristen für Wahlpflichtfächer.

**Beispiel-Relationen:** Zwischen diesen Knoten gibt es sinnvolle Beziehungen, zum Beispiel: *“Studiengang enthält Modul”*, *“Modul wird gelehrt von Ansprechpartner”*, *“Studiengang hat Abschlussgrad (B.Sc./M.Sc.)”*, *“Ansprechpartner hat Rolle (Professor, Mitarbeiter)”* oder *“Studiengang hat Bewerbungsfrist”*. Konkret könnte der Graph z. B. ausdrücken: *Studiengang Informatik – umfasst – Modul Programmieren I*, *Prof. Müller – ist Ansprechpartner für – Studiengang Informatik*, oder *Vertiefung Data Science – Wahlzeitraum_start – Januar 2025*. Durch solche Kanten wird implizites Wissen explizit modelliert.

Ein Website-Wissensgraph verfolgt typischerweise zwei Hauptziele: **(a)** Inhalte der Website *vollständig und korrekt* abbilden (möglichst jedes relevante Datum wird erfasst), und **(b)** eine *Ontologie* (Schemastruktur) bereitstellen, die diese Inhalte formal beschreibt (z. B. Klassen wie *Studiengang*, *Modul*, *Person* und Relationen wie *hatModul*, *betreutStudiengang* etc.). Mit einer solchen Ontologie kann der Graph validiert und erweitert werden, und externe Daten (etwa aus anderen Hochschulquellen) könnten angebunden werden.

## 3. Warum Automatisierung wichtig ist

Die Inhalte einer Hochschul-Webseite ändern sich laufend – neue **Meldungen**, aktualisierte **Studienordnungen**, personelle Änderungen bei **Ansprechpartnern** oder geänderte **Termine**. Eine **automatisierte** Erstellung und Aktualisierung des Wissensgraphen ist daher essentiell, um *laufend* den aktuellen Stand abzubilden, ohne jedes Mal händisch Daten einpflegen zu müssen. Manuelle Pflege eines Wissensgraphen wäre nicht nur zeitaufwendig, sondern auch fehleranfällig. Automation stellt sicher, dass die Daten **konsistent und aktuell** bleiben, während der Aufwand für Wartung minimiert wird. Traditionell erfordert die Erstellung eines Wissensgraphen viel fachliches Know-how und Handarbeit, doch moderne Ansätze – etwa mit NLP und KI – können große Teile des Prozesses übernehmen.

Wichtige Gründe für Automatisierung:

- **Aktualität:** Änderungen auf der Website (z. B. neue News, geänderte Fristen) sollen zeitnah im Graph reflektiert werden. Ein Crawler kann z. B. täglich die Seite durchsuchen und Differenzen erkennen.
- **Wartungsaufwand senken:** Ist der Prozess einmal eingerichtet, läuft er weitgehend selbstständig. Redakteure müssen nicht doppelt arbeiten (Inhalt pflegen *und* Daten für den Graphen liefern), sondern der Graph zieht die Informationen aus der bestehenden Webseite.
- **Delta-Erkennung:** Ein gutes System erkennt **neue oder veränderte Inhalte** automatisch. Zum Beispiel könnte der Crawler beim Vergleich mit dem letzten Lauf feststellen, dass eine neue Person in der Mitarbeiterliste hinzugekommen ist oder dass eine Studiengangsseite aktualisiert wurde. Solche Deltas können markiert und – nach Bestätigung – in den Graph übernommen werden.
- **Validierung vor Einfügen:** Automatisierung heißt nicht ungeprüfte Übernahme. Es ist wichtig, neu erkannte Fakten zunächst zu *validieren*. Dies kann teils automatisiert (etwa durch Schema-Validierung mittels SHACL) geschehen, teils durch kurze manuelle Prüfung. Automatisiert erkannte Inhalte sollten idealerweise von einem Skript oder einem Redakteur gegengeprüft werden, bevor sie endgültig in den Wissensgraph geschrieben werden. Dadurch bleibt die **Datenqualität** hoch.
- **Qualitätssicherung (QA):** Mit einem stets aktuellen Wissensgraphen lassen sich auch Qualitätssicherungs-Abfragen formulieren. Beispiel: Gibt es einen Studiengang auf der Website ohne zugeordneten Ansprechpartner im Graph? Solche Anomalien deuten auf Erfassungsprobleme hin. Ein automatisierter Prozess kann solche Fälle melden.

Insgesamt ermöglicht eine Automatisierung, dass der Wissensgraph **lebendig** bleibt und mit der Website mitwächst, ohne dass dafür erheblicher Mehraufwand entsteht. Die Redakteure können sich auf die Inhaltspflege konzentrieren, während der technische Prozess die Graph-Aktualisierung übernimmt.

## 4. Technische Grundlagen und Architektur

Um einen Wissensgraphen aus einer Website wie fiw.thws.de automatisiert zu erzeugen, bedarf es einer **mehrstufigen Pipeline** mit verschiedenen Komponenten. Im Folgenden werden die Hauptschritte einer möglichen Architektur erläutert, alle vorzugsweise basierend auf dem **Python-Ökosystem** für leichte Wartbarkeit und Open-Source-Unterstützung:

1. **Crawler (Web Scraping):** Zunächst wird ein Crawler benötigt, der die Website **vollständig durchsucht und die HTML-Inhalte** einsammelt. Tools wie *Scrapy* bieten hierfür ein leistungsfähiges Gerüst – man definiert Regeln für zu besuchende URLs und zu extrahierende Inhalte, den Rest erledigt das Framework. Für dynamische Inhalte (falls Teile der Seite via JavaScript nachgeladen werden) kann ein Headless-Browser wie *Playwright* oder *Selenium* integriert werden, um die gerenderte Seite auszulesen. Der Crawler sollte so konfiguriert sein, dass er alle relevanten Unterseiten der fiw.thws.de findet (z. B. durch Folgen aller internen Links oder Auslesen einer Sitemap falls vorhanden). Ergebnis dieses Schritts: eine Sammlung von HTML-Seiten bzw. der rohe Inhalt aller Seiten.

1. **Parser & Content-Extractor:** Die rohen HTML-Seiten müssen nun verarbeitet werden, um die **relevanten Daten** zu isolieren. Hier kommen Parser wie *BeautifulSoup* (Python library zur HTML-Analyse) zum Einsatz, oder man nutzt direkt Scrapy’s Selektoren. Beispielsweise kann der Parser aus einer Personen-Liste die Namen, E-Mail-Adressen und Räume extrahieren, oder aus einer Studiengangsseite die Liste der Module, Abschlussart, Dauer etc. Dieser Schritt erfordert oft individuelles *Parsing pro Seitentyp* (z. B. anders für News-Meldungen als für Personen-Profile). Das Resultat ist strukturierte Zwischen-Daten, z. B. in Form von Python-Dictionaries oder DataFrames, die z. B. folgendermaßen aussehen könnten: `{ "Studiengang": "Informatik", "Abschluss": "B.Eng.", "Module": ["Programmieren I", "..."], "Ansprechpartner": "Prof. X" }`.

1. **NLP & Entity Recognition:** Viele Inhalte sind in Fließtext eingebettet. Ein **Natural Language Processing**-Schritt kann helfen, Entitäten und Relationen aus solchen Texten zu ziehen. Mit *spaCy*, einer Open-Source-NLP-Bibliothek in Python ([spaCy 101: Everything you need to know · spaCy Usage Documentation](https://spacy.io/usage/spacy-101#:~:text=spaCy%20is%20a%20free%2C%20open,NLP%29%20in%20Python)), lassen sich **Named Entities** erkennen, etwa Personennamen, Organisationen oder Orte. Auf einer Seite mit einer News-Meldung könnten so Namen von Personen oder Institutionen identifiziert werden. Falls komplexe Satzstrukturen vorhanden sind, kann *Open Information Extraction (OpenIE)* eingesetzt werden, um aus Sätzen automatisch Tripel (Subjekt, Prädikat, Objekt) zu extrahieren ([GitHub - philipperemy/stanford-openie-python: Stanford Open Information Extraction made simple!](https://github.com/philipperemy/stanford-openie-python#:~:text=Open%20information%20extraction%20,Obama%3B%20was%20born%20in%3B%20Hawaii)). OpenIE-Tools identifizieren z. B. aus dem Satz *„Prof. Müller leitet den Studiengang Informatik“* das Tripel *(Prof. Müller; leitet; Studiengang Informatik)*, ohne dass vordefinierte Schemas nötig sind ([GitHub - philipperemy/stanford-openie-python: Stanford Open Information Extraction made simple!](https://github.com/philipperemy/stanford-openie-python#:~:text=Open%20information%20extraction%20,Obama%3B%20was%20born%20in%3B%20Hawaii)). Dieser NLP-Schritt ergänzt die strukturierten Daten aus dem vorherigen Schritt und findet Beziehungen, die nicht offensichtlich aus dem HTML-Layout extrahierbar waren.

1. **Mapping zu Ontologie (Daten-Mapping):** Die extrahierten Informationen müssen nun auf das **Schema (Ontologie)** des Wissensgraphen abgebildet werden. Dazu definiert man eine Ontologie, also Klassen und Relationen, die im Graph existieren dürfen (z. B. Klasse *Studiengang*, Relation *hatModul* etc.). Das Mapping kann teils regelbasiert erfolgen (z. B. wenn im Parser schon klar ist, dass ein bestimmtes Feld ein Modulname ist, wird daraus eine Entität vom Typ *Modul*). In komplizierteren Fällen helfen Mapping-Tools oder man schreibt Zuordnungsregeln: z. B. *„Wenn ein Person-Name erkannt wurde und Kontext ‘Professor’ auftaucht, instanziiere Knoten vom Typ Professor und verknüpfe mit zugehörigem Studiengang.“* Automatische Ontologie-Erstellung ist ebenfalls denkbar (etwa mit Hilfe von KI, die aus den Daten ein Schema vorschlägt), doch gerade in einem abgegrenzten Domain wie Hochschulwebseite ist es meist besser, initial ein schlankes, manuell erstelltes Schema vorzugeben und dieses bei Bedarf zu erweitern.

1. **Graph-Erzeugung (Triple Creation):** Nun werden aus den gemappten Daten **Tripel bzw. Graph-Kanten** erzeugt. Mit *RDFLib*, einer Python-Bibliothek für RDF-Manipulation, lassen sich RDF-Tripel erstellen, in gängige Formate (Turtle, RDF/XML etc.) serialisieren und in ein RDF-Repository laden. Alternativ kann man die Daten auch direkt via API in eine Graph-Datenbank schreiben (z. B. mit Treibern für Neo4j, falls man ein Property-Graph-Modell nutzt). In diesem Schritt entsteht der eigentliche **Knowledge Graph**: Jede extrahierte Entität wird zu einem Knoten im Graph, und für jede erkannte Beziehung wird eine Kante zwischen den entsprechenden Knoten eingefügt. Zum Beispiel wird die Entität *„Prof. Dr. Tobias Aubele“* als Knoten vom Typ *Person* angelegt, die Entität *„Studiengang Informatik“* als Knoten vom Typ *Studiengang*, und durch das Mapping weiß das System, dass Prof. Aubele der Studiengangsleiter ist – folglich wird eine Kante *leitet->* von *Aubele* zu *Informatik* hinzugefügt.

1. **Speicherung im Triple-Store / Graph-DB:** Für Abfragen und die dauerhafte Nutzung muss der Graph in einer entsprechenden **Datenbank** gespeichert werden. Hier gibt es zwei Hauptansätze: Ein RDF-Triplestore (wie z. B. *GraphDB* oder Apache Jena Fuseki) der auf SPARQL als Abfragesprache setzt, **oder** eine Property-Graph-Datenbank wie *Neo4j*, welche eine eigene Abfragesprache (Cypher) verwendet. GraphDB ist ein RDF-Store von Ontotext, der sich gut für ontologie-basierte Daten eignet (inkl. inferenzbasierter Abfragen). Neo4j ist eine populäre Graph-DB, die vor allem für hoch performante Traversal-Abfragen und flexible Property-Graph-Modelle bekannt ist. Beide Varianten können mit Python angesteuert werden: RDFLib kann auch als In-Memory-Triplestore dienen und via SPARQL abgefragt werden, während für Neo4j z. B. die **neo4j Python Driver** oder ORMs wie *py2neo* genutzt werden können. Entscheidend ist, dass das System **regelmäßige Updates** unterstützt – also entweder den Graph inkrementell aktualisiert (neue Tripel hinzufügen, alte bei Bedarf entfernen) oder bei jedem Lauf neu aufbaut (kleinere Graphen lassen sich schnell neu generieren).

1. **Validierung und Qualitätssicherung:** Bevor die neu gewonnenen Daten endgültig übernommen werden, sollte ein *Validierungsmodul* prüfen, ob die Tripel zur Ontologie passen und ob keine offensichtlichen Fehler vorliegen. *pySHACL* kann z. B. verwendet werden, um den Graph gegen definierte SHACL-Regeln zu prüfen (Shapes Constraint Language – eine Art Schema für RDF), und etwa zu melden, wenn ein Pflichtattribut fehlt oder ein Datentyp nicht stimmt. Diese Validierung stellt sicher, dass nur konsistente und brauchbare Informationen im Wissensgraph landen.

Diese Architektur lässt sich so aufsetzen, dass jeder Schritt automatisiert abläuft, aber dennoch transparent und debug-freundlich bleibt. Durch Logging kann nachvollzogen werden, welche neuen Entitäten erkannt wurden und welche Änderungen erfolgt sind. Im Kern greift diese Pipeline auf etablierte Technologien zurück – von Web Scraping über NLP bis zum Semantic Web Stack – und ist so gestaltet, dass sie modular erweiterbar ist (z. B. könnte man später einen **Übersetzungsservice** ergänzen, um zweisprachige Knoten *Deutsch/Englisch* für Studiengangsnamen zu erzeugen).

## 5. Anbieter, Tools und Bibliotheken

Für jede der obigen Phasen gibt es bewährte **Tools** und **Bibliotheken**. Im Folgenden eine kurze Übersicht der empfohlenen Lösungen – möglichst als Open-Source und in Python nutzbar – gegliedert nach Aufgabenschritt:

- **Crawler & Extraktion:**

  - **Scrapy:** Ein populäres, schnelles Web-Scraping-Framework in Python. Ermöglicht das effiziente Crawlen ganzer Websites und das Extrahieren strukturierter Daten anhand von Regeln. Ideal für statische Seiten mit konsistentem HTML-Aufbau.
  - **Playwright (mit Python):** Ein moderner Headless-Browser von Microsoft, der über Python angesprochen werden kann. Eignet sich für dynamische oder JavaScript-lastige Seiten, da er die Seite wie ein echter Browser rendert. Kann z. B. genutzt werden, um die Navigation auf fiw.thws.de nachzuahmen, wenn bestimmte Inhalte erst nach Klick sichtbar werden.
  - **Diffbot:** Kommerzielle API, die KI-gestützt beliebige Webseiten in strukturierte JSON-Daten umwandelt. *Diffbot* **automatisiert** die Web-Datenextraktion und transformiert unstrukturierten Web-Content in durchsuchbare, strukturierte Daten ([AI-Powered Web Data Extraction | Diffbot](https://tooldirectory.ai/tools/diffbot-ai-web-data-extraction#:~:text=Diffbot%20automates%20web%20data%20extraction,knowledge%20graph%20of%20web%20information)). Es erkennt Artikel, Produkte, Personen etc. per KI (Computer Vision, NLP) und füllt damit einen eigenen Knowledge Graph. Vorteil: Wenig eigener Aufwand; Nachteil: kostenpflichtig und Blackbox. Für ein Open-Source-Projekt würde man eher Scrapy + eigene Logik vorziehen, aber Diffbot zeigt, was technisch möglich ist.

- **NLP & Semantik:**

  - **spaCy:** Eine leistungsfähige NLP-Bibliothek in Python, optimiert für den Produktionseinsatz. Sie bietet schnelle **Named Entity Recognition (NER)**, Part-of-Speech-Tagging, Dependency Parsing u.v.m. out-of-the-box ([spaCy 101: Everything you need to know · spaCy Usage Documentation](https://spacy.io/usage/spacy-101#:~:text=spaCy%20is%20a%20free%2C%20open,NLP%29%20in%20Python)). Mit dem deutschen Modell kann spaCy z. B. Namen von Personen, Orten, Organisationen aus Texten der Website erkennen.

  - **Haystack (deepset):** Ein Open-Source-Framework aus Deutschland, das speziell für **QA-Systeme und semantische Suche** entwickelt wurde. Haystack ermöglicht das Bauen von *Retrieval-Augmented Generation*-(RAG) Pipelines, kombiniert also dokumentenbasiertes Retrieval mit generativen LLMs (\[

    What is Haystack? | Haystack

  \](https://haystack.deepset.ai/overview/intro#:~:text=Haystack%20is%20an%C2%A0open%20source%20framework%C2%A0for,flexible%20and%20easy%20to%20use)). Für unseren Zweck lässt sich Haystack z. B. einsetzen, um Fragen der Nutzer gegen den Wissensgraph (oder die zugrundeliegenden Texte) zu beantworten. Außerdem bietet es Module zum Vorverarbeiten von Texten, Indexieren und komplexe Pipeline-Orchestrierung.

  - **Transformers (Hugging Face):** Die *Transformers*-Bibliothek stellt vortrainierte Sprachmodelle (BERT, GPT-ähnlich, etc.) bereit. Damit kann man z. B. spezialisierte NER oder Relation Extraction durchführen, falls spaCy nicht genügt. Ein feingetunter BERT könnte z. B. erkennen, dass *„leitet“* im Kontext *„Prof. X leitet Studiengang Y“* eine *Leiter-von-Beziehung* darstellt. Transformers sind vor allem dann nützlich, wenn man sehr domänenspezifische Extraktionen automatisieren möchte.
  - **OpenIE:** Open Information Extraction-Tools wie Stanford OpenIE oder kleinere Python-Implementierungen (z. B. `stanford-openie-python`). Sie extrahieren **Tripel** direkt aus Sätzen ([GitHub - philipperemy/stanford-openie-python: Stanford Open Information Extraction made simple!](https://github.com/philipperemy/stanford-openie-python#:~:text=Open%20information%20extraction%20,Obama%3B%20was%20born%20in%3B%20Hawaii)). Dies ist hilfreich, um Relationen zu finden, ohne sie manuell definieren zu müssen. Allerdings können die Ergebnisse linguistisch bedingt auch Rauschen enthalten – man sollte OpenIE-Ausgaben ggf. durch Nachbearbeitung oder Validierungsregeln filtern.

- **Graph-Erzeugung & -Speicherung:**

  - **RDFLib:** Eine reine Python-Bibliothek zur Arbeit mit RDF-Daten. Sie kann Tripel speichern, SPARQL-Abfragen ausführen, RDF-Dateien (TTL, XML usw.) einlesen und schreiben. Im Kontext unserer Aufgabe kann RDFLib genutzt werden, um während der Extraktion Tripel anzulegen und am Ende des Prozesses den gesamten Graphen als Turtle-Datei zu serialisieren. Auch das Laden in einen lokalen Triple-Store oder das Absetzen von SPARQL-Updates ist möglich.
  - **pySHACL:** Ein Python-Validator für SHACL (Shapes Constraint Language). Er erlaubt die Validierung von RDF-Graphen gegen ein Regelwerk (Shapes), das die Ontologie-Bedingungen abbildet ([pyshacl - PyPI](https://pypi.org/project/pyshacl/0.9.9/#:~:text=pyshacl%20,graphs%20against%20Shapes%20Constraint)). Beispielsweise kann man definieren: *Jeder Knoten vom Typ Studiengang muss mindestens einen Ansprechpartner haben.* pySHACL prüft dann den Graphen und meldet Verstöße. Dieses Tool ist wichtig für die Qualitätssicherung, bevor man automatisiert Daten einspielt.
  - **Neo4j:** Eine hochperformante Graphdatenbank, die das **Property-Graph-Modell** benutzt. Knoten und Kanten können dabei beliebige Eigenschaften (Properties) haben. Abfragen erfolgen in Cypher, einer deklarativen Abfragesprache. Neo4j eignet sich, wenn man viele relationale Abfragen hat (z. B. *„Zeige alle Module, die über Prof. X mit Studiengang Y verbunden sind“*) und ggf. kürzere Entwicklungszyklen – es ist sehr visualisierungsfreundlich (z. B. via Neo4j Browser). Python-Anbindung per offiziellem Neo4j-Driver oder ORMs. Für unseren Wissensgraph könnte Neo4j eine Alternative zu RDF sein; man würde dann statt RDF-Klassen Label verwenden (z. B. Label *Studiengang* am Knoten).
  - **GraphDB (Ontotext):** Ein kommerzieller, aber in begrenztem Umfang auch frei nutzbarer RDF-Triplestore, der sich in vielen Enterprise-Anwendungen bewährt hat. GraphDB unterstützt SPARQL 1.1, Inferenz (Regeln, OWL-Unterstützung) und Skalierung auf große Graphen. In Python kann man GraphDB über SPARQL-Endpunkte ansprechen (z. B. mittels `SPARQLWrapper`-Bibliothek). Für unsere Zwecke würde GraphDB den Vorteil bieten, dass er direkt RDF und Ontologie-Logik versteht (z. B. Subklassen oder Synonyme, sollte man sowas benötigen).

- **Automatische Ontologie-Erstellung / Mapping:**

  - **SKOS:** Steht für *Simple Knowledge Organization System*, ein W3C-Standard, um kontrollierte Vokabularien bzw. Thesauri als RDF zu repräsentieren. SKOS ist zwar keine Ontologie-Mapping-Software, aber hilfreich, um beispielsweise Kategorien oder begriffliche Hierarchien der Website abzubilden (etwa Fakultät > Studiengänge > Module). Sollten Fachbegriffe oder Tags auf fiw.thws.de vorkommen, könnte man sie in SKOS-Konzeptschemen überführen.
  - **D2RQ:** Ein älteres, aber bekanntes Tool, das relationale Datenbanken auf RDF abbildet (Mapping-Rules). Falls die Webseite ihre Daten aus einer Datenbank zieht und man direkten DB-Zugriff hätte, könnte man D2RQ einsetzen, um daraus einen RDF-Graphen zu generieren, ohne den Umweg über Crawling. In unserem Szenario gehen wir aber vom Web-Crawling aus; D2RQ sei der Vollständigkeit halber erwähnt als Mapping-Option für strukturierte Datenquellen.
  - **Karma (USC):** Eine Open-Source-Software der University of Southern California für **semi-automatisches Daten-Mapping** auf Ontologien. Mit einer GUI kann man Beispiel-Daten (CSV, JSON, Web-APIs etc.) auf Ontologieklassen ziehen, und Karma lernt daraus ein Modell. Dieses kann man exportieren und auf neue Daten anwenden ([usc-isi-i2/Web-Karma: Information Integration Tool - GitHub](https://github.com/usc-isi-i2/Web-Karma#:~:text=Karma%20learns%20to%20recognize%20the,Users%20then)). Denkbar wäre, Karma zu nutzen, um anhand einiger annotierter Webseiten-Abschnitte ein Mapping zu erstellen – z. B. markiert man in HTML-Tabellen, welche Spalte der *Modulname* ist, welche der *Dozent*, etc., und Karma könnte daraus eine Zuordnung zur Ontologie (Klasse *Modul*, Beziehung *hatDozent*) generieren. Im Betrieb würde man aber wahrscheinlich eher direkt in Python die Regeln codieren; Karma ist dennoch nützlich, um das Ontologie-Mapping initial zu beschleunigen.
  - **Weitere:** Es gibt noch andere Tools wie *OpenRefine (mit RDF-Extension)* für manuelles Mapping, oder *OntoRefine* (Teil von GraphDB Workbench) um CSV/JSON zu RDF zu transformieren. Für unser Vorhaben sind diese aber weniger zentral, da wir direkt aus HTML extrahieren. Sollte die THWS-Webseite jedoch über Schnittstellen (RSS-Feeds, APIs) verfügen, könnte man diese Tools ergänzend einsetzen.

## 6. Best Practices & Empfehlungen

Bei der Umsetzung eines solchen automatisierten Website-Knowledge-Graphs sollten einige **Best Practices** beachtet werden, um Langlebigkeit, Qualität und Akzeptanz des Systems sicherzustellen:

- **Klare Ontologie & Strukturvorgaben:** Definieren Sie *vorab* ein Schema, welche Klassen und Relationen es geben soll. Eine gut durchdachte Ontologie ist das Rückgrat des Wissensgraphen. Sie sollte die Inhalte der Website passend modellieren (nicht zu fein, nicht zu grob). Halten Sie sich an Standards, wo möglich (z. B. `schema.org` für Personen oder Organisationen), damit Ihr Graph auch extern verständlich ist.
- **Iterative Entwicklung:** Starten Sie mit einem **MVP (Minimum Viable Product)** – beispielsweise zuerst nur die **Studiengänge und zugehörigen Professoren** erfassen. Testen Sie das Durchlaufen der Pipeline, und erweitern Sie schrittweise um weitere Entitäten (Module, Veranstaltungen, News etc.). So bleibt das Projekt überschaubar, und frühe Nutzer können Feedback geben.
- **Validierung & Regeln:** Nutzen Sie technische Validierung (z. B. SHACL mit pySHACL) und definieren Sie Regeln, um Fehler abzufangen. Beispiel: Wenn ein Modul ohne zugehörigen Studiengang extrahiert wurde, stimmt etwas nicht – solche Datensätze sollten nicht importiert werden, bevor geklärt ist, ob es sich um einen Fehler handelt. Fangen Sie auch technische Probleme ab (z. B. HTML-Änderungen auf der Website können den Parser brechen – hier sollte der Crawler entsprechende Exceptions loggen, damit Entwickler sie sehen und Anpassungen vornehmen).
- **Monitoring & Delta-Reports:** Richten Sie ein Monitoring ein, das z. B. wöchentliche **Änderungsberichte** erzeugt. Eine Liste aller neu hinzugekommenen oder geänderten Knoten/Relationen kann an die Redakteure geschickt werden oder in einem Dashboard landen. So behalten alle den Überblick und können unerwartete Änderungen prüfen.
- **Rückkopplung mit Redakteuren:** Beziehen Sie die inhaltlich Verantwortlichen der Website mit ein. Zum einen können diese helfen, die Ontologie festzulegen (welche Begriffe verwendet man offiziell? Welche Beziehungen sind relevant?). Zum anderen sollte es einen **Feedback-Kanal** geben: Falls der Wissensgraph Fehler enthält (weil etwa Inhalte zweideutig waren), müssen Redakteure entweder den Graph anpassen oder idealerweise die Website so anpassen, dass der Crawler es beim nächsten Mal richtig macht. Dieses Zusammenspiel erhöht die Datenqualität auf beiden Seiten.
- **Versionskontrolle der Daten:** Behandeln Sie die Wissensgraph-Daten ähnlich wie Code: Es lohnt sich, *Versionen* der extrahierten RDF-Daten in einem Repository abzulegen (ggf. in JSON-LD oder Turtle Format). So kann man Änderungen über die Zeit nachvollziehen und notfalls auf einen vorherigen Stand zurück. Außerdem erleichtert es Entwicklern die Arbeit, wenn Beispieldaten zur Verfügung stehen, an denen sie Verbesserungen testen können.
- **Umgang mit Links und IDs:** Verwenden Sie **stabile Identifier** für Entitäten im Graph. Ideal ist es, wenn die Website bereits IDs bietet (z. B. in URLs oder als HTML-IDs). Diese kann man als URI für die Knoten benutzen, um Konsistenz zu wahren. Wenn die Seite keinen eindeutigen Schlüssel liefert, muss man einen generieren (z. B. aus Name + Typ, was aber bei Namensänderungen zu Duplikaten führen kann). Planen Sie auch eine Strategie für gelöschte Inhalte: Wenn z. B. ein Studiengang von der Webseite entfernt wird (eingestellter Studiengang), soll der entsprechende Knoten im Graph bestehen bleiben (ggf. mit Attribut *status: inaktiv*) oder entfernt werden? Solche Richtlinien vermeiden später Inkonsistenzen.
- **Performance und Skalierung:** Eine Fakultätswebsite ist von der Datenmenge her überschaubar, aber achten Sie trotzdem auf effiziente Verarbeitung. Cachen Sie Ergebnisse, wo sinnvoll (z. B. heruntergeladene Seiten zwischenspeichern, falls sie sich selten ändern). Setzen Sie den Crawler nicht zu aggressiv an (respektieren Sie `robots.txt` und setzen Sie Pausen zwischen Requests, um den Webserver nicht zu überlasten).
- **Sicherheit und Datenschutz:** Da Personeninformationen verarbeitet werden (Ansprechpartner mit Kontaktdaten), stellen Sie sicher, dass der Umgang datenschutzkonform ist. Zwar sind die Daten öffentlich auf der Website, aber bei weiterer Verwendung (z. B. in einem Chatbot) sollten keine über die Quelle hinausgehenden Personenprofile erstellt werden. Halten Sie sich an geltende Richtlinien der Hochschule zur Datenverarbeitung.

Durch Befolgung dieser Best Practices erhöhen Sie die **Robustheit** Ihres Systems. Das bedeutet: Weniger Ausfälle bei Website-Änderungen, konsistente Daten trotz Automatisierung, und Akzeptanz bei den Nutzern (sowohl intern – Redakteure, Entscheider – als auch extern – Studierende, Chatbot-Anwender). Langfristig soll der Wissensgraph ein zuverlässiges Abbild der Fakultätsinformationen liefern.

## 7. Fazit & Ausblick

Die automatisierte Erstellung eines Knowledge Graphen für die Website fiw.thws.de ist mit heutigen Mitteln **realistisch umsetzbar**. Schon jetzt gibt es alle benötigten Komponenten als Open-Source-Bausteine: Vom Scrapy-Crawler über spaCy-NLP bis hin zu RDFLib und einer Graphdatenbank. Ein mögliches **MVP** (Minimalprodukt) könnte sich zunächst auf Kerninformationen beschränken, z. B. *Studiengänge*, *zugehörige Module* und *Ansprechpartner*. Damit ließe sich bereits ein einfacher Prototyp eines **QA-Chatbots** speisen, der Fragen beantwortet wie *„Wer ist der Studiengangsleiter von E-Commerce?“* oder *„Welche Module gibt es im 5. Semester Informatik?“*.

In weiteren Ausbaustufen kann die **Ontologie laufend verfeinert** werden – etwa um Konzepte wie *Labore*, *Forschungsprojekte*, *Veranstaltungen* oder *Alumni-Netzwerk* zu integrieren, die auf der Website erwähnt werden. Neue Inhaltstypen der Webseite (z. B. ein neu eingeführter Studiengang oder eine neue Nachrichtenrubrik) können durch den modularen Aufbau relativ leicht ins Mapping aufgenommen werden. Langfristig wäre es interessant, den Graphen auch mit anderen Datenquellen zu verknüpfen – z. B. mit dem zentralen **Hochschul-Knowledge-Graph** der THWS oder mit öffentlichen Wissensgraphen (etwa GND für Personen oder DBpedia). So könnten z. B. Professorenprofile automatisch mit Publikationen verlinkt werden.

Ein wichtiges zukünftiges Feature ist die **automatische Erkennung neuer Inhalte**: Sobald Redakteure auf fiw.thws.de eine neue Seite anlegen oder eine bestehende ändern, sollte der Crawler dies erkennen. Hier kann man mit *Change Detection* arbeiten – entweder via Timestamp-Prüfung (letztes Änderungsdatum der Seite) oder inhaltsbasiertem Hash-Vergleich. In Kombination mit einem Benachrichtigungssystem würde das bedeuten, dass keine Erweiterung der Website übersehen wird.

Für die Nutzung des Wissensgraphen in einem **RAG-Chatbot** (Retrieval-Augmented Generation) zeichnet sich ein spannender Ausblick ab. Tools wie Haystack erlauben bereits heute die Kombination von Wissensgraph-Abfragen mit generativen KI-Modellen (\[

```
  What is Haystack? | Haystack
```

\](https://haystack.deepset.ai/overview/intro#:~:text=,or%20build%20a%20knowledge%20graph)). Denkbar ist, den Graphen in Vektorsuche und semantische Indizierung einzubeziehen, sodass ein Chatbot komplexe Fragen beantworten kann, indem er zuerst relevante Fakten aus dem Graphen zieht und diese dann in eine Sprachmodell-Antwort einbettet (\[

```
  What is Haystack? | Haystack
```

\](https://haystack.deepset.ai/overview/intro#:~:text=,or%20build%20a%20knowledge%20graph)). Ein Beispiel: *„Welche Veranstaltungen wurden im Sommer 2024 von der Fakultät berichtet?“* – der Bot könnte via SPARQL alle News-Knoten zum Sommer 2024 finden und deren Inhalte dann zusammenfassen. Die Integration von Wissensgraph und KI wird derzeit intensiv erforscht, und Ihr Projekt könnte von diesen Fortschritten profitieren.

**Fazit:** Mit vertretbarem Aufwand lässt sich aus der Fakultätswebsite fiw.thws.de ein nachhaltiger, mehrwertstiftender Wissensgraph erzeugen. Wichtig ist, sauber und automatisiert vorzugehen, aber dennoch menschliche Kontrolle einzuplanen. So entsteht schrittweise ein **lebendiges Wissensnetz**, das Studenten, Mitarbeitern und Systemen (wie Chatbots oder Suchanwendungen) gleichermaßen dient. Heute machbar ist ein funktionierender Graph mit Grunddaten; als nächste Schritte winken intelligente Aktualisierungsmechanismen, breitere Ontologie-Abdeckung und die Kopplung mit modernen KI-Services, um das volle Potenzial der Daten auszuschöpfen.
